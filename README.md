2023/08/07

科研项目代码备份：

研究方向：测井曲线预测、时间序列预测、岩性识别

2023/09/11：新增使用Transformer实现油井曲线预测

2023/09/13：
实验1：Decoder中Mask机制使用原始Transformer
实验2：Decoder中Mask机制使用None，效果提升3倍
实验3：在实验二的基础上修改滑动窗口为：滑动窗口大小为5，使用左右两边各2个值预测中间一个值，效果同实验二差不多，在一个量级上
实验4：在输入特征上添加岩性特征，预测密度
实验5-1：根据岩性分类，训练三个岩性自己对应的模型
实验5-2：根据测试集中岩性占比情况，选用适合自己的模型，此实验仅预测不训练
实验6: 添加归一化:Z-Score归一化效果更好，调大滑动窗口至25，预测井4的GR曲线
实验7：在encoder层之前添加CNN提取特征

2023/09/18：
实验8：想复现MWLT模型 **失败**
实验9：实验6的基础上调参，**结果不好**
实验10：成功复现MWLT网络，可预测自己的测井数据集
实验11：将MWLT的Decoder部分改为传统的Transformer的Decoder，Mask设置为None
实验12: 修改网络Position Encoding部分
实验13：集成学习实验，训练三种岩性的不同模型  **效果巨差**
实验14：集成学习实验，用单个井的数据集做集成学习实验

2023/09/29：
实验15: 想办法提升模型精度

2023/10/8：路径：Transformer/code/traditional
对比实验：
LSTM1.py：LSTM进行测井曲线预测（拟合）
LSTM2.py：增加output数量实现未来测井数据预测（效果不好）
LSTM3.py：递归的方式实现未来测井数据预测
LSTM+CNN.py：LSTM+CNN网络实现预测
GRU1.py：GRU网络进行测井曲线预测

2023/10/9：
实验15: 改进MWLT网络（修改构建序列方式）
实验16：在MWLT上结合ConvTransformer中的Local Attention机制:在Transformer前加入Conv（已有）、修改Decoder部分为conv+sigmoid
实验17：在MWLT上修改Decoder部分为简单的线性层/增加rescnn层数为6




每个实验均重复五次，计算平均的MSE、和MAE
实验一：
MSE：（0.020209715+ 0.45176294 + 0.18956667 +0.033740446 +0.026158333）/ 5 =0.1442876208
MAE：（0.09856577 +0.59654516 + 0.3308607 + 0.1256848 + 0.11821458）/ 5 = 0.253974202
实验二：
MSE：（0.0291+0.09016076+0.082779445+0.017700052+0.022508625）/ 5 = 0.0484497764
MAE：（0.1230+0.18922842+0.20687115+0.10252892+0.10459467）/ 5 = 0.145244632
实验三：
MSE：（0.019035773 + 0.017292082 + 0.10452878 + 0.048319645 +0.07834819 ）/ 5 = 0.053504894
MAE：（0.09913854 + 0.0936903 + 0.26748022 + 0.13733713 +0.14204061）/ 5 =0.14793736
实验四：
MSE：（0.020140467 + 0.03669662 + 0.018531872 + 0.031116439 +0.013731226 ）/ 5 = 0.0240433248
MAE：（0.11346615 + 0.12805888 +0.09715088 + 0.14565593 +0.08238643）/ 5 = 0.113343654
实验5-2
MSE：0.042026833 + 0.016688105
MAE：0.12391149 + 0.09635593
实验6：
平均绝对误差（MAE）： 0.71574336   0.7135172
均方误差（MSE）： 0.9349156  0.89616877

实验7：
平均绝对误差（MAE）： 1.0230266  1.0727686
均方误差（MSE）： 1.6964855  1.5557811
 
实验7 GR：
平均绝对误差（MAE）： 1.1516125
均方误差（MSE）： 1.6873156

实验6 GR：
窗口大小25：
平均绝对误差（MAE）： 0.8539464 
均方误差（MSE）： 1.311501
窗口大小100：
平均绝对误差（MAE）： 0.9113283
均方误差（MSE）： 1.5046527

实验6 DENSITY：
测试集：
平均绝对误差（MAE）： 0.9690845  1.113121
均方误差（MSE）： 1.708891  2.1923325
训练：
平均绝对误差（MAE）： 0.52187186  0.50057113
均方误差（MSE）： 0.73657805  0.69805044

实验7 DENSITY：
测试集：
平均绝对误差（MAE）： 0.9627114  0.97743285
均方误差（MSE）： 1.6430182  1.6165
训练：
平均绝对误差（MAE）： 0.6705363  0.6095604
均方误差（MSE）： 0.9673783  0.84413797

实验7 井1 GR
训练：
平均绝对误差（MAE）： 0.47199762
均方误差（MSE）： 0.39229605
测试：
平均绝对误差（MAE）： 1.6670914
均方误差（MSE）： 4.7643666

实验10：井4  epoch 100   MWLTDecoder  原始位置编码
平均绝对误差（MAE）： (0.07980831 + 0.077441014 + 0.07445156) / 3 = 0.077233628
均方误差（MSE）： (0.010471328 + 0.010972441 + 0.009240652) / 3 = 0.01022814033

实验10：井4  epoch 100   MWLTDecoder  相对位置编码
平均绝对误差（MAE）： (0.107839935 + 0.0823988 + 0.081696525) / 3 = 0.09064508667
均方误差（MSE）： (0.01698751 + 0.013093341 + 0.011384744) / 3 = 0.013821865

实验11：井4  epoch 100   torch.nn.TransformerDecoder 原始位置编码
平均绝对误差（MAE）： (0.09502743 + 0.10253625 + 0.08941058) / 3 = 0.09565808667
均方误差（MSE）： (0.01301257 + 0.014833678 + 0.01274404) / 3 = 0.013530096

位置编码参考：https://github.com/Navidfoumani/ConvTran/blob/main/Models/Attention.py
实验12：井4 epoch 100  torch.nn.TransformerDecoder  绝对位置编码tAPE
平均绝对误差（MAE）： (0.08284263 + 0.100381054 + 0.08097428) / 3 = 0.088065988
均方误差（MSE）： (0.00986363 + 0.01436855 + 0.010764684) / 3 = 0.01166562133

实验12：井4 epoch 100  torch.nn.TransformerDecoder  可学习的位置编码LearnablePositionalEncoding
平均绝对误差（MAE）： (0.093834504 + 0.09154032 + 0.08126485) / 3 = 0.08887989133
均方误差（MSE）： (0.012920193 + 0.012394508 + 0.009890943) / 3 = 0.01173521467

实验12：井4 epoch 100  torch.nn.TransformerDecoder  相对位置编码
平均绝对误差（MAE）： (0.09452269 + 0.09405877 + 0.08040875) / 3 = 0.08966340333
均方误差（MSE）： (0.012080684 + 0.012129836 + 0.00938971) / 3 = 0.01120007667

实验14：井4 epoch 60 单井集成学习实验   用后50%做测试集预测
平均绝对误差（MAE）： 0.087857336
均方误差（MSE）： 0.01429429

实验14：井4 epoch 60 单井集成学习实验   用后20%做测试集预测
平均绝对误差（MAE）： 0.08079494
均方误差（MSE）： 0.011779512

实验15：
MSE: 0.017013629898428917
RMSE: 0.13043630123138428
MAE: 0.09695392102003098
MAPE: 0.8746445775032043

实验16：
MSE: 0.020150410011410713
RMSE: 0.14195214211940765
MAE: 0.10518769919872284
MAPE: 0.8505666851997375

实验17：
修改Decoder为简单的线性层：
MSE: 0.02025354653596878
RMSE: 0.14231495559215546
MAE: 0.10433118790388107
MAPE: 0.8039875030517578
更改rescnn层数为6：
MSE: 0.018315182998776436
RMSE: 0.1353335976600647
MAE: 0.10110252350568771
MAPE: 0.8914665579795837

